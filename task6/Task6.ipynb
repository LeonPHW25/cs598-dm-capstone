{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Sometimes we make decisions beyond the rating of a restaurant. For example, if a restaurant has a high rating but it often fails to pass hygiene inspections, then this information can dissuade many people to eat there. Using this hygiene information could lead to a more informative system; however, it is often the case where we don’t have such information for all the restaurants, and we are left to make predictions based on the small sample of data points.\n",
    "\n",
    "In this task, you are going to predict whether a set of restaurants will pass the public health inspection tests given the corresponding Yelp text reviews along with some additional information such as the locations and cuisines offered in these restaurants. Making a prediction about an unobserved attribute using data mining techniques represents a wide range of important applications of data mining. Through working on this task, you will gain direct experience with such an application. Due to the flexibility of using as many indicators for prediction as possible, this would also give you an opportunity to potentially combine many different algorithms you have learned from the courses in the Data Mining Specialization to solve a real world problem and experiment with different methods to understand what’s the most effective way of solving the problem.\n",
    "\n",
    "## About the Dataset\n",
    "You should first download the [dataset](https://d396qusza40orc.cloudfront.net/dataminingcapstone/Task6/Hygiene.tar.gz). The dataset is composed of a training subset containing 546 restaurants used for training your classifier, in addition to a testing subset of 12753 restaurants used for evaluating the performance of the classifier. In the training subset, you will be provided with a binary label for each restaurant, which indicates whether the restaurant has passed the latest public health inspection test or not, whereas for the testing subset, you will not have access to any labels. The dataset is spread across three files such that the first 546 lines in each file corresponding to the training subset, and the rest are part of the testing subset. Below is a description of each file:\n",
    "\n",
    "* **hygiene.dat**: Each line contains the concatenated text reviews of one restaurant.  \n",
    "* **hygiene.dat.labels**: For the first 546 lines, a binary label (0 or 1) is used where a 0 indicates that the restaurant has passed the latest public health inspection test, while a 1 means that the restaurant has failed the test. The rest of the lines have \"[None]\" in their label field implying that they are part of the testing subset.\n",
    "* **hygiene.dat.additional**: It is a CSV (Comma-Separated Values) file where the first value is a list containing the cuisines offered, the second value is the zip code, which gives an idea about the location, the third is the number of reviews, and the fourth is the average rating, which can vary between 0 and 5 (5 being the best).\n",
    "\n",
    "For testing, we use the F1 measure, which is the harmonic mean of precision and recall, to rank the submissions in the leaderboard. The F1 measure will be based on the macro-averages of precision and recall (macro-averaging is used here to ensure that the two classes are given equal weight as we do not want class 0 to dominate the measure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hygiene_text_path= \"./Hygiene/hygiene.dat\"\n",
    "hygiene_labels_path= \"./Hygiene/hygiene.dat.labels\"\n",
    "hygiene_additional_path= \"./Hygiene/hygiene.dat.additional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from UtilWordEmbedding import MeanEmbeddingVectorizer\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_numeric, strip_short\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces, strip_non_alphanum, remove_stopwords, stem_text\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "SEED=26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Download Glove Embeddings\n",
    "```\n",
    "wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "unzip glove.6B.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Prep\n",
    "Preprocessing Steps:\n",
    "* tokenization and cleaning\n",
    "* stopword removal\n",
    "* stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and preprocess\n",
    "# https://radimrehurek.com/gensim/parsing/preprocessing.html\n",
    "FILTERS_LIST = [lambda x: x.lower(), # lowercase  \n",
    "                strip_tags, # remove tags\n",
    "                strip_punctuation, # replace punctuation characters with spaces\n",
    "                strip_multiple_whitespaces, # remove repeating whitespaces\n",
    "                # strip_numeric, # remove numbers\n",
    "                gensim.parsing.preprocessing.remove_stopwords, # remove stopwords\n",
    "                strip_short, # remove words less than minsize=3 characters long]\n",
    "                stem_text]\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, \n",
    "    \"\"\"\n",
    "    result_stemmed = []\n",
    "    for token in gensim.parsing.preprocessing.preprocess_string(text, FILTERS_LIST):\n",
    "        result_stemmed.append(WordNetLemmatizer().lemmatize(token))\n",
    "    return result_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13299/13299 [01:33<00:00, 142.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 732 ms, total: 1min 33s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = []\n",
    "preprocessed_texts = []\n",
    "\n",
    "with open(hygiene_text_path) as f:\n",
    "    texts = f.readlines()\n",
    "    \n",
    "for _text in tqdm(texts):\n",
    "    result_stemmed = preprocess(_text)\n",
    "    preprocessed_texts.append(result_stemmed)\n",
    "    \n",
    "all_preprocessed_texts = [\" \".join(_text) for _text in preprocessed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13299 entries, 0 to 13298\n",
      "Data columns (total 8 columns):\n",
      "label                 13299 non-null object\n",
      "text                  13299 non-null object\n",
      "preprocessed_texts    13299 non-null object\n",
      "tokenized_texts       13299 non-null object\n",
      "cuisines_offered      13299 non-null object\n",
      "zipcode               13299 non-null object\n",
      "num_reviews           13299 non-null object\n",
      "avg_rating            13299 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 831.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>tokenized_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The baguettes and rolls are excellent, and alt...</td>\n",
       "      <td>baguett roll excel haven tri excit dozen plu t...</td>\n",
       "      <td>[baguett, roll, excel, haven, tri, excit, doze...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I live up the street from Betty. &amp;#160;When my...</td>\n",
       "      <td>live street betti 160 sister town spring break...</td>\n",
       "      <td>[live, street, betti, 160, sister, town, sprin...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm worried about how I will review this place...</td>\n",
       "      <td>worri review place strongli think bad night pl...</td>\n",
       "      <td>[worri, review, place, strongli, think, bad, n...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Why can't you access them on Google street vie...</td>\n",
       "      <td>access googl street view like medina yarrow po...</td>\n",
       "      <td>[access, googl, street, view, like, medina, ya...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Things to like about this place: homemade guac...</td>\n",
       "      <td>thing like place homemad guacamol varieti tast...</td>\n",
       "      <td>[thing, like, place, homemad, guacamol, variet...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text                                 preprocessed_texts                                    tokenized_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0     1  The baguettes and rolls are excellent, and alt...  baguett roll excel haven tri excit dozen plu t...  [baguett, roll, excel, haven, tri, excit, doze...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1     1  I live up the street from Betty. &#160;When my...  live street betti 160 sister town spring break...  [live, street, betti, 160, sister, town, sprin...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2     1  I'm worried about how I will review this place...  worri review place strongli think bad night pl...  [worri, review, place, strongli, think, bad, n...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3     0  Why can't you access them on Google street vie...  access googl street view like medina yarrow po...  [access, googl, street, view, like, medina, ya...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4     0  Things to like about this place: homemade guac...  thing like place homemad guacamol varieti tast...  [thing, like, place, homemad, guacamol, variet...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 546\n",
    "\n",
    "# labels \n",
    "with open(hygiene_labels_path, 'r') as f:\n",
    "    labels = [l.rstrip() for l in f]\n",
    "\n",
    "# texts = []\n",
    "# with open(hygiene_text_path, 'r') as f:\n",
    "#     texts = f.read().splitlines(True)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"label\":labels, \"text\": texts, \n",
    "                   \"preprocessed_texts\": all_preprocessed_texts,\n",
    "                   \"tokenized_texts\": preprocessed_texts})\n",
    "hygiene_additional = pd.read_csv(hygiene_additional_path,  \n",
    "                                 names=[\"cuisines_offered\", \"zipcode\", \"num_reviews\", \"avg_rating\"],\n",
    "                                 dtype={\"cuisines_offered\": str, \n",
    "                                        \"zipcode\": str,\n",
    "                                        \"num_reviews\": str})\n",
    "df = df.join(hygiene_additional)\n",
    "df['avg_rating'] = df['avg_rating'].apply(lambda x: str(int(round(x, 0))))\n",
    "\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 5) (546, 5) (546, 5) (546,)\n",
      "(12753, 5) (12753, 5) (12753, 5) (12753,)\n",
      "text                object\n",
      "cuisines_offered    object\n",
      "zipcode             object\n",
      "num_reviews         object\n",
      "avg_rating          object\n",
      "dtype: object preprocessed_texts    object\n",
      "cuisines_offered      object\n",
      "zipcode               object\n",
      "num_reviews           object\n",
      "avg_rating            object\n",
      "dtype: object tokenized_texts     object\n",
      "cuisines_offered    object\n",
      "zipcode             object\n",
      "num_reviews         object\n",
      "avg_rating          object\n",
      "dtype: object\n",
      "CPU times: user 16.1 ms, sys: 1.95 ms, total: 18.1 ms\n",
      "Wall time: 16.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = df[df[\"label\"] != \"[None]\"]\n",
    "test_df = df[df[\"label\"] == \"[None]\"]\n",
    "\n",
    "additional_feats = [\"cuisines_offered\", \"zipcode\", \"num_reviews\", \"avg_rating\"]\n",
    "\n",
    "train = train_df[[\"text\"] + additional_feats]\n",
    "train_preprocessed = train_df[[\"preprocessed_texts\"] + additional_feats]\n",
    "train_tokenized = train_df[[\"tokenized_texts\"] + additional_feats]\n",
    "train_labels = train_df[\"label\"].astype(int) # needed by sklearn\n",
    "\n",
    "test = test_df[[\"text\"] + additional_feats]\n",
    "test_preprocessed = test_df[[\"preprocessed_texts\"] + additional_feats]\n",
    "test_tokenized = test_df[[\"tokenized_texts\"] + additional_feats]\n",
    "test_labels = test_df[\"label\"]\n",
    "\n",
    "print(train.shape, train_preprocessed.shape, train_tokenized.shape, train_labels.shape)\n",
    "print(test.shape, test_preprocessed.shape, test_tokenized.shape, test_labels.shape)\n",
    "print(train.dtypes, train_preprocessed.dtypes, train_tokenized.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The baguettes and rolls are excellent, and alt...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I live up the street from Betty. &amp;#160;When my...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I'm worried about how I will review this place...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Why can't you access them on Google street vie...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Things to like about this place: homemade guac...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  The baguettes and rolls are excellent, and alt...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  I live up the street from Betty. &#160;When my...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  I'm worried about how I will review this place...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  Why can't you access them on Google street vie...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  Things to like about this place: homemade guac...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>baguett roll excel haven tri excit dozen plu t...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>live street betti 160 sister town spring break...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>worri review place strongli think bad night pl...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>access googl street view like medina yarrow po...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>thing like place homemad guacamol varieti tast...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  preprocessed_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  baguett roll excel haven tri excit dozen plu t...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  live street betti 160 sister town spring break...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  worri review place strongli think bad night pl...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  access googl street view like medina yarrow po...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  thing like place homemad guacamol varieti tast...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[baguett, roll, excel, haven, tri, excit, doze...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[live, street, betti, 160, sister, town, sprin...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[worri, review, place, strongli, think, bad, n...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[access, googl, street, view, like, medina, ya...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[thing, like, place, homemad, guacamol, variet...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     tokenized_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  [baguett, roll, excel, haven, tri, excit, doze...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  [live, street, betti, 160, sister, town, sprin...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  [worri, review, place, strongli, think, bad, n...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  [access, googl, street, view, like, medina, ya...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  [thing, like, place, homemad, guacamol, variet...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.head())\n",
    "display(train_preprocessed.head())\n",
    "display(train_tokenized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just use cross_val_score\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_preprocessed, train_labels, test_size= 0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Experiments\n",
    "\n",
    "Models:  \n",
    "* Naive Bayes\n",
    "* SVM\n",
    "* Logistic Regression\n",
    "* Random Forest\n",
    "* XGBoost\n",
    "\n",
    "Feature Engineering:  \n",
    "* Count Vectorizer\n",
    "* Tfidf Vectorizer\n",
    "* Previously Generated Embeddings\n",
    "    * word2vec\n",
    "    * fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62857143 0.71153846 0.64912281 0.62711864 0.55319149]\n",
      "Average F1-Score: 0.63391\n",
      "CPU times: user 14.9 s, sys: 404 ms, total: 15.3 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "         ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), 'preprocessed_texts')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', SVC(probability=True))\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# scores = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train_preprocessed, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "         ('text', TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                strip_accents='unicode',\n",
    "                min_df=15,\n",
    "                max_df=0.5,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500), 'text')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# score = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def test_classifier(clf, X, y, vectorizer, text_col='text'):\n",
    "    pipeline = Pipeline([\n",
    "        ('union', ColumnTransformer(\n",
    "            [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "             ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "             ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "             ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "             ('text', vectorizer, text_col)],\n",
    "            remainder='passthrough',\n",
    "        )),\n",
    "        ('clf', clf)\n",
    "    ], verbose=False)\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring= 'f1')\n",
    "    print(clf)\n",
    "    print(scores)\n",
    "    cv_score = np.average(scores)\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.46153846 0.48780488 0.51111111 0.55172414 0.33333333]\n",
      "CPU times: user 18 s, sys: 544 ms, total: 18.5 s\n",
      "Wall time: 18.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4691023843925442"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfidf = TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=15,\n",
    "                    max_df=0.3,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500)\n",
    "bow = CountVectorizer(\n",
    "                stop_words=STOP_WORDS,\n",
    "                strip_accents='unicode',\n",
    "                min_df=5,\n",
    "                max_df=0.3,\n",
    "                ngram_range=(1, 3))\n",
    "test_classifier(clf=svm.SVC(), \n",
    "                X=train, y=train_labels, \n",
    "                vectorizer=bow, text_col='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Support Vector Machine': svm.SVC(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=15,\n",
    "                    max_df=0.3,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500)\n",
    "bow = CountVectorizer(\n",
    "                stop_words=STOP_WORDS,\n",
    "                strip_accents='unicode',\n",
    "                min_df=5,\n",
    "                max_df=0.3,\n",
    "                ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 BOW - No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.58823529 0.65420561 0.69724771 0.63551402 0.56565657]\n",
      "Naive Bayes: 0.6281718384728912\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.46153846 0.48780488 0.51111111 0.55172414 0.33333333]\n",
      "Support Vector Machine: 0.4691023843925442\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.61111111 0.54545455 0.61682243 0.66666667 0.58947368]\n",
      "Logistic Regression: 0.6059056874698783\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[0.69026549 0.6        0.62711864 0.63247863 0.62962963]\n",
      "Random Forest: 0.6358984785803445\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "[0.65       0.56363636 0.55238095 0.63551402 0.54945055]\n",
      "Gradient Boosting: 0.5901963768318909\n",
      "CPU times: user 1min 34s, sys: 3.29 s, total: 1min 38s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train, train_labels, \n",
    "                               vectorizer=bow, text_col='text')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 BOW - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.58       0.71559633 0.69026549 0.64220183 0.56      ]\n",
      "Naive Bayes: 0.6376127303726556\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.44155844 0.49382716 0.50549451 0.55172414 0.35135135]\n",
      "Support Vector Machine: 0.46879111936583195\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.60784314 0.5625     0.59813084 0.71559633 0.63157895]\n",
      "Logistic Regression: 0.6231298512040097\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[0.68376068 0.64957265 0.62385321 0.63793103 0.55445545]\n",
      "Random Forest: 0.6299146048739642\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "[0.64347826 0.60194175 0.62264151 0.66037736 0.55555556]\n",
      "Gradient Boosting: 0.6167988863844929\n",
      "CPU times: user 1min 13s, sys: 2.5 s, total: 1min 16s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train_preprocessed, train_labels, \n",
    "                               vectorizer=bow, text_col='preprocessed_texts')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TFIDF - No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.62264151 0.69090909 0.66071429 0.64220183 0.52272727]\n",
      "Naive Bayes: 0.6278387987293994\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.64150943 0.71153846 0.66071429 0.63247863 0.55319149]\n",
      "Support Vector Machine: 0.6398864606110692\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.61946903 0.7037037  0.65517241 0.63247863 0.48351648]\n",
      "Logistic Regression: 0.6188680520081192\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[0.62608696 0.57627119 0.5840708  0.59459459 0.62135922]\n",
      "Random Forest: 0.600476551463632\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "[0.65546218 0.56862745 0.53846154 0.60194175 0.50505051]\n",
      "Gradient Boosting: 0.5739086853878401\n",
      "CPU times: user 1min 14s, sys: 1.99 s, total: 1min 16s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train, train_labels, \n",
    "                               vectorizer=tfidf, text_col='text')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TFIDF - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.64150943 0.7027027  0.64912281 0.63636364 0.52272727]\n",
      "Naive Bayes: 0.630485170554684\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.61538462 0.72380952 0.64912281 0.62711864 0.55319149]\n",
      "Support Vector Machine: 0.6337254159282363\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.61261261 0.71028037 0.63793103 0.62184874 0.52083333]\n",
      "Logistic Regression: 0.6207012187512557\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[0.66101695 0.63461538 0.57142857 0.64347826 0.60606061]\n",
      "Random Forest: 0.6233199544253339\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "[0.66086957 0.55769231 0.56074766 0.67256637 0.50526316]\n",
      "Gradient Boosting: 0.5914278132074507\n",
      "CPU times: user 1min 3s, sys: 1.77 s, total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train_preprocessed, train_labels, \n",
    "                               vectorizer=tfidf, text_col='preprocessed_texts')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Additional Attempts\n",
    "Word Embeddings: Gensim word2vec\n",
    "Ensemble: mlens\n",
    "\n",
    "To try in future:\n",
    "* Undersampling\n",
    "* fasttext\n",
    "* BERT\n",
    "* Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  if __name__ == '__main__':\n",
      "WARNING:root:cannot compute average owing to no vector for []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 1.21 s, total: 2min 29s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "print(num_workers)\n",
    "\n",
    "word_model = Word2Vec(preprocessed_texts, \n",
    "                      size=300, window=5, min_count=3, \n",
    "                      workers=num_workers)\n",
    "w2v = {w: vec for w, vec in zip(word_model.wv.index2word, word_model.wv.syn0)}\n",
    "mean_vec_tr = MeanEmbeddingVectorizer(word_model)\n",
    "w2v_embeddings = mean_vec_tr.transform(preprocessed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 300)\n",
      "(12753, 300)\n",
      "(546, 8)\n",
      "(12753, 8)\n"
     ]
    }
   ],
   "source": [
    "w2v_train = w2v_embeddings[:546]\n",
    "w2v_test = w2v_embeddings[546:]\n",
    "print(w2v_train.shape)\n",
    "print(w2v_test.shape)\n",
    "      \n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57425743 0.66       0.67272727 0.64       0.5625    ]\n",
      "Average F1-Score: 0.62190\n",
      "CPU times: user 356 ms, sys: 1.86 ms, total: 358 ms\n",
      "Wall time: 357 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_train = w2v_embeddings[:546]\n",
    "clf = svm.SVC()# XGBClassifier(n_estimators=500, n_jobs=-1)\n",
    "scores = cross_val_score(clf, w2v_train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61946903 0.65517241 0.61538462 0.65       0.56074766]\n",
      "Average F1-Score: 0.62015\n",
      "CPU times: user 11.7 s, sys: 18.6 ms, total: 11.7 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_train = w2v_embeddings[:546]\n",
    "params = {'max_depth': 6, \n",
    "         'eta': 0.3, \n",
    "         'objective': 'binary:logistic', \n",
    "         'subsample':0.8, \n",
    "         'n_estimators': 500,\n",
    "         'scale_pos_weight': 2,\n",
    "         'eval_metric': 'auc',\n",
    "         'n_jobs': -1}\n",
    "clf = XGBClassifier(n_estimators=500, \n",
    "                    max_depth=5, \n",
    "                    learning_rate=0.2, \n",
    "                    objective='binary:logistic',\n",
    "                    scale_pos_weight=2,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=SEED)\n",
    "scores = cross_val_score(clf, w2v_train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Ensemble Using mlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1(y, p): return f1_score(y, p, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=2,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
       "   random_state=4917, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, folds=2, raise_on_ex...8e20d0>)],\n",
       "   n_jobs=-1, name='group-16', raise_on_exception=True, transformers=[])],\n",
       "   verbose=0)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=26, sample_size=20,\n",
       "       scorer=<function f1 at 0x1a468e20d0>, shuffle=False, verbose=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Build ---\n",
    "\n",
    "# Passing a scoring function will create cv scores during fitting\n",
    "# the scorer should be a simple function accepting to vectors and returning a scalar\n",
    "ensemble = SuperLearner(scorer=f1, random_state=SEED)\n",
    "\n",
    "# Build the first layer\n",
    "layer1_classifiers = [MultinomialNB(),\n",
    "                      SVC(probability=False),\n",
    "                      LogisticRegression(),\n",
    "                      XGBClassifier(n_estimators=500, \n",
    "                            max_depth=5, \n",
    "                            learning_rate=0.2, \n",
    "                            objective='binary:logistic',\n",
    "                            scale_pos_weight=2,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=SEED),\n",
    "                      RandomForestClassifier(random_state=SEED, n_estimators=250, n_jobs=-1)]\n",
    "ensemble.add(layer1_classifiers)\n",
    "\n",
    "# Attach the final meta estimator\n",
    "ensemble.add_meta(LogisticRegression())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59405941 0.68965517 0.64347826 0.64957265 0.61386139]\n",
      "Average F1-Score: 0.63813\n",
      "CPU times: user 1min 6s, sys: 5.84 s, total: 1min 12s\n",
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "         ('text', TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                strip_accents='unicode',\n",
    "                min_df=15,\n",
    "                max_df=0.5,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500), 'text')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', ensemble)\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# score = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.2 s, sys: 1.33 s, total: 40.5 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline.fit(train, train_labels)\n",
    "y_pred = pipeline.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path ='./submissions/submission7_mlens1.txt'\n",
    "create_submission(y_pred, submit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_pred, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('jc26\\n')\n",
    "        for label in y_pred:\n",
    "            f.write(str(int(label)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 184 ms, total: 15.7 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "         ('text', TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                strip_accents='unicode',\n",
    "                min_df=3,\n",
    "                max_df=0.5,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500), 'preprocessed_texts')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "pipeline.fit(train_preprocessed, train_labels)\n",
    "y_pred = pipeline.predict(test_preprocessed)\n",
    "# score = metrics.f1_score(y_test, y_pred)\n",
    "# scores = cross_val_score(pipeline, train, train_labels, cv=5, scoring= 'f1')\n",
    "# print(scores)\n",
    "# print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path ='./submissions/submission6_NB_preprocessed.txt'\n",
    "create_submission(y_pred, submit_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Check Before Submitting!!!\n",
    "And just run the follwing cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./submissions/submission7_mlens1.txt'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission completed successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!python submit.py jc26 {submit_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Method Comparison\n",
    "Tried two or more text representation techniques and two or more learning algorithms and used the additional features; The comparison gives an insight on why some methods perform better than others.\n",
    "\n",
    "* Text Representation: How text is represented as features (for example, unigrams is one technique, bigrams is another)\n",
    "* Learning Algorithm: The classification algorithm used (for example, SVM, Naive Bayes, Logistic Regression, ... )\n",
    "\n",
    "\n",
    "### Best Performing Method\n",
    "* What toolkit was used?\n",
    "* How was text preprocessed? (i.e., stopword removal, stemming, or any data cleaning technique)\n",
    "* How was text represented as features?\n",
    "* What was the learning algorithm used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html#sphx-glr-auto-examples-compose-plot-column-transformer-py\n",
    "* Used MeanEmbeddingVectorizer from these articles:\n",
    "    * https://towardsdatascience.com/nlp-performance-of-different-word-embeddings-on-text-classification-de648c6262b\n",
    "    * http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "* https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
