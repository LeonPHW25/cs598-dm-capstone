{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Sometimes we make decisions beyond the rating of a restaurant. For example, if a restaurant has a high rating but it often fails to pass hygiene inspections, then this information can dissuade many people to eat there. Using this hygiene information could lead to a more informative system; however, it is often the case where we don’t have such information for all the restaurants, and we are left to make predictions based on the small sample of data points.\n",
    "\n",
    "In this task, you are going to predict whether a set of restaurants will pass the public health inspection tests given the corresponding Yelp text reviews along with some additional information such as the locations and cuisines offered in these restaurants. Making a prediction about an unobserved attribute using data mining techniques represents a wide range of important applications of data mining. Through working on this task, you will gain direct experience with such an application. Due to the flexibility of using as many indicators for prediction as possible, this would also give you an opportunity to potentially combine many different algorithms you have learned from the courses in the Data Mining Specialization to solve a real world problem and experiment with different methods to understand what’s the most effective way of solving the problem.\n",
    "\n",
    "## About the Dataset\n",
    "You should first download the [dataset](https://d396qusza40orc.cloudfront.net/dataminingcapstone/Task6/Hygiene.tar.gz). The dataset is composed of a training subset containing 546 restaurants used for training your classifier, in addition to a testing subset of 12753 restaurants used for evaluating the performance of the classifier. In the training subset, you will be provided with a binary label for each restaurant, which indicates whether the restaurant has passed the latest public health inspection test or not, whereas for the testing subset, you will not have access to any labels. The dataset is spread across three files such that the first 546 lines in each file corresponding to the training subset, and the rest are part of the testing subset. Below is a description of each file:\n",
    "\n",
    "* **hygiene.dat**: Each line contains the concatenated text reviews of one restaurant.  \n",
    "* **hygiene.dat.labels**: For the first 546 lines, a binary label (0 or 1) is used where a 0 indicates that the restaurant has passed the latest public health inspection test, while a 1 means that the restaurant has failed the test. The rest of the lines have \"[None]\" in their label field implying that they are part of the testing subset.\n",
    "* **hygiene.dat.additional**: It is a CSV (Comma-Separated Values) file where the first value is a list containing the cuisines offered, the second value is the zip code, which gives an idea about the location, the third is the number of reviews, and the fourth is the average rating, which can vary between 0 and 5 (5 being the best).\n",
    "\n",
    "For testing, we use the F1 measure, which is the harmonic mean of precision and recall, to rank the submissions in the leaderboard. The F1 measure will be based on the macro-averages of precision and recall (macro-averaging is used here to ensure that the two classes are given equal weight as we do not want class 0 to dominate the measure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hygiene_text_path= \"./Hygiene/hygiene.dat\"\n",
    "hygiene_labels_path= \"./Hygiene/hygiene.dat.labels\"\n",
    "hygiene_additional_path= \"./Hygiene/hygiene.dat.additional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_numeric, strip_short\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces, strip_non_alphanum, remove_stopwords, stem_text\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "SEED=26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in Data\n",
    "\n",
    "Preprocessing Steps:\n",
    "* tokenization and cleaning\n",
    "* stopword removal\n",
    "* stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and preprocess\n",
    "# https://radimrehurek.com/gensim/parsing/preprocessing.html\n",
    "FILTERS_LIST = [lambda x: x.lower(), # lowercase  \n",
    "                strip_tags, # remove tags\n",
    "                strip_punctuation, # replace punctuation characters with spaces\n",
    "                strip_multiple_whitespaces, # remove repeating whitespaces\n",
    "                # strip_numeric, # remove numbers\n",
    "                gensim.parsing.preprocessing.remove_stopwords, # remove stopwords\n",
    "                strip_short, # remove words less than minsize=3 characters long]\n",
    "                stem_text]\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, \n",
    "    \"\"\"\n",
    "    result_stemmed = []\n",
    "    for token in gensim.parsing.preprocessing.preprocess_string(text, FILTERS_LIST):\n",
    "        result_stemmed.append(WordNetLemmatizer().lemmatize(token))\n",
    "    return result_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13299/13299 [01:34<00:00, 140.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 700 ms, total: 1min 35s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "texts = []\n",
    "preprocessed_texts = []\n",
    "\n",
    "with open(hygiene_text_path) as f:\n",
    "    texts = f.readlines()\n",
    "    \n",
    "for _text in tqdm(texts):\n",
    "    result_stemmed = preprocess(_text)\n",
    "    preprocessed_texts.append(result_stemmed)\n",
    "    \n",
    "all_preprocessed_texts = [\" \".join(_text) for _text in preprocessed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13299 entries, 0 to 13298\n",
      "Data columns (total 7 columns):\n",
      "label                 13299 non-null object\n",
      "text                  13299 non-null object\n",
      "preprocessed_texts    13299 non-null object\n",
      "cuisines_offered      13299 non-null object\n",
      "zipcode               13299 non-null object\n",
      "num_reviews           13299 non-null object\n",
      "avg_rating            13299 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 727.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The baguettes and rolls are excellent, and alt...</td>\n",
       "      <td>baguett roll excel haven tri excit dozen plu t...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I live up the street from Betty. &amp;#160;When my...</td>\n",
       "      <td>live street betti 160 sister town spring break...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm worried about how I will review this place...</td>\n",
       "      <td>worri review place strongli think bad night pl...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Why can't you access them on Google street vie...</td>\n",
       "      <td>access googl street view like medina yarrow po...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Things to like about this place: homemade guac...</td>\n",
       "      <td>thing like place homemad guacamol varieti tast...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text                                 preprocessed_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0     1  The baguettes and rolls are excellent, and alt...  baguett roll excel haven tri excit dozen plu t...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1     1  I live up the street from Betty. &#160;When my...  live street betti 160 sister town spring break...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2     1  I'm worried about how I will review this place...  worri review place strongli think bad night pl...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3     0  Why can't you access them on Google street vie...  access googl street view like medina yarrow po...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4     0  Things to like about this place: homemade guac...  thing like place homemad guacamol varieti tast...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 546\n",
    "\n",
    "# labels \n",
    "with open(hygiene_labels_path, 'r') as f:\n",
    "    labels = [l.rstrip() for l in f]\n",
    "\n",
    "# texts = []\n",
    "# with open(hygiene_text_path, 'r') as f:\n",
    "#     texts = f.read().splitlines(True)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"label\":labels, \"text\": texts, \"preprocessed_texts\": all_preprocessed_texts})\n",
    "hygiene_additional = pd.read_csv(hygiene_additional_path,  \n",
    "                                 names=[\"cuisines_offered\", \"zipcode\", \"num_reviews\", \"avg_rating\"],\n",
    "                                 dtype={\"cuisines_offered\": str, \n",
    "                                        \"zipcode\": str,\n",
    "                                        \"num_reviews\": str})\n",
    "df = df.join(hygiene_additional)\n",
    "df['avg_rating'] = df['avg_rating'].apply(lambda x: str(int(round(x, 0))))\n",
    "\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 5) (546, 5) (546,)\n",
      "(12753, 5) (12753, 5) (12753,)\n",
      "text                object\n",
      "cuisines_offered    object\n",
      "zipcode             object\n",
      "num_reviews         object\n",
      "avg_rating          object\n",
      "dtype: object preprocessed_texts    object\n",
      "cuisines_offered      object\n",
      "zipcode               object\n",
      "num_reviews           object\n",
      "avg_rating            object\n",
      "dtype: object\n",
      "CPU times: user 131 ms, sys: 58.9 ms, total: 190 ms\n",
      "Wall time: 188 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = df[df[\"label\"] != \"[None]\"]\n",
    "test_df = df[df[\"label\"] == \"[None]\"]\n",
    "\n",
    "train = train_df.drop([\"label\", \"preprocessed_texts\"], axis=1)\n",
    "train_preprocessed = train_df.drop([\"label\", \"text\"], axis=1)\n",
    "train_labels = train_df[\"label\"].astype(int) # needed by sklearn\n",
    "\n",
    "test = test_df.drop([\"label\", \"preprocessed_texts\"], axis=1)\n",
    "test_preprocessed = test_df.drop([\"label\", \"text\"], axis=1)\n",
    "test_labels = test_df[\"label\"]\n",
    "\n",
    "print(train.shape, train_preprocessed.shape, train_labels.shape)\n",
    "print(test.shape, test_preprocessed.shape, test_labels.shape)\n",
    "print(train.dtypes, train_preprocessed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The baguettes and rolls are excellent, and alt...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I live up the street from Betty. &amp;#160;When my...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I'm worried about how I will review this place...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Why can't you access them on Google street vie...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Things to like about this place: homemade guac...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  The baguettes and rolls are excellent, and alt...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  I live up the street from Betty. &#160;When my...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  I'm worried about how I will review this place...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  Why can't you access them on Google street vie...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  Things to like about this place: homemade guac...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>baguett roll excel haven tri excit dozen plu t...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>live street betti 160 sister town spring break...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>worri review place strongli think bad night pl...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>access googl street view like medina yarrow po...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>thing like place homemad guacamol varieti tast...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  preprocessed_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  baguett roll excel haven tri excit dozen plu t...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  live street betti 160 sister town spring break...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  worri review place strongli think bad night pl...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  access googl street view like medina yarrow po...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  thing like place homemad guacamol varieti tast...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.head())\n",
    "display(train_preprocessed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just use cross_val_score\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_preprocessed, train_labels, test_size= 0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Experiments\n",
    "\n",
    "Models:  \n",
    "* Naive Bayes\n",
    "* SVM\n",
    "* Logistic Regression\n",
    "* Random Forest\n",
    "* XGBoost\n",
    "\n",
    "Feature Engineering:  \n",
    "* Count Vectorizer\n",
    "* Tfidf Vectorizer\n",
    "* word embedding: GloVe and fastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62857143 0.7184466  0.64912281 0.62711864 0.55319149]\n",
      "Average F1-Score: 0.63529\n",
      "CPU times: user 12.3 s, sys: 381 ms, total: 12.7 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "         ('text', TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                strip_accents='unicode',\n",
    "                min_df=15,\n",
    "                max_df=0.5,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500), 'preprocessed_texts')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', svm.SVC())\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# scores = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train_preprocessed, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62857143 0.69811321 0.65486726 0.62608696 0.55319149]\n",
      "Average F1-Score: 0.63217\n",
      "CPU times: user 14.8 s, sys: 490 ms, total: 15.3 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "         ('text', TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                strip_accents='unicode',\n",
    "                min_df=25,\n",
    "                max_df=0.5,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500), 'text')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', svm.SVC())\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# score = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 22.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def test_classifier(clf, X_train, y_train, text_col='text'):\n",
    "    pipeline = Pipeline([\n",
    "        ('union', ColumnTransformer(\n",
    "            [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "             ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "             ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "             ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "             ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=15,\n",
    "                    max_df=0.3,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), text_col)],\n",
    "            remainder='passthrough',\n",
    "        )),\n",
    "        ('clf', clf)\n",
    "    ], verbose=False)\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring= 'f1')\n",
    "    print(clf)\n",
    "    print(scores)\n",
    "    cv_score = np.average(scores)\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Support Vector Machine': svm.SVC(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.62264151 0.69090909 0.66071429 0.64220183 0.52272727]\n",
      "Naive Bayes: 0.6278387987293994\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.64150943 0.71153846 0.66071429 0.63247863 0.55319149]\n",
      "Support Vector Machine: 0.6398864606110692\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.61946903 0.7037037  0.65517241 0.63247863 0.48351648]\n",
      "Logistic Regression: 0.6188680520081192\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[0.63716814 0.63157895 0.56880734 0.63551402 0.60194175]\n",
      "Random Forest: 0.6150020389350574\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "[0.66666667 0.58823529 0.55045872 0.57943925 0.58252427]\n",
      "Gradient Boosting: 0.5934648401123506\n",
      "CPU times: user 1min 16s, sys: 3.09 s, total: 1min 19s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train, train_labels)\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.64150943 0.7027027  0.64912281 0.63636364 0.52272727]\n",
      "Naive Bayes: 0.630485170554684\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.61538462 0.72380952 0.64912281 0.62711864 0.55319149]\n",
      "Support Vector Machine: 0.6337254159282363\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.61261261 0.71028037 0.63793103 0.62184874 0.52083333]\n",
      "Logistic Regression: 0.6207012187512557\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[0.64864865 0.66666667 0.63793103 0.63865546 0.58585859]\n",
      "Random Forest: 0.6355520795683067\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "[0.66666667 0.55238095 0.59813084 0.69642857 0.50526316]\n",
      "Gradient Boosting: 0.6037740378984845\n",
      "CPU times: user 1min 3s, sys: 2.46 s, total: 1min 6s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train_preprocessed, train_labels, text_col='preprocessed_texts')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Attempts\n",
    "* Undersampling\n",
    "* FastText Word Embedding w/ Gensim\n",
    "* BERT\n",
    "* NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_pred, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('jc26\\n')\n",
    "        for label in y_pred:\n",
    "            f.write(str(label) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "         ('text', TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                strip_accents='unicode',\n",
    "                min_df=25,\n",
    "                max_df=0.5,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500), 'text')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', svm.SVC())\n",
    "], verbose=False)\n",
    "\n",
    "pipeline.fit(train, train_labels)\n",
    "y_pred = pipeline.predict(test)\n",
    "# score = metrics.f1_score(y_test, y_pred)\n",
    "# scores = cross_val_score(pipeline, train, train_labels, cv=5, scoring= 'f1')\n",
    "# print(scores)\n",
    "# print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path ='./submissions/submission2_SVC.txt'\n",
    "create_submission(y_pred, submit_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Check Before Submitting!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python submit.py jc26 {submit_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Method Comparison\n",
    "Tried two or more text representation techniques and two or more learning algorithms and used the additional features; The comparison gives an insight on why some methods perform better than others.\n",
    "\n",
    "* Text Representation: How text is represented as features (for example, unigrams is one technique, bigrams is another)\n",
    "* Learning Algorithm: The classification algorithm used (for example, SVM, Naive Bayes, Logistic Regression, ... )\n",
    "\n",
    "\n",
    "### Best Performing Method\n",
    "* What toolkit was used?\n",
    "* How was text preprocessed? (i.e., stopword removal, stemming, or any data cleaning technique)\n",
    "* How was text represented as features?\n",
    "* What was the learning algorithm used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html#sphx-glr-auto-examples-compose-plot-column-transformer-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
