{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Sometimes we make decisions beyond the rating of a restaurant. For example, if a restaurant has a high rating but it often fails to pass hygiene inspections, then this information can dissuade many people to eat there. Using this hygiene information could lead to a more informative system; however, it is often the case where we don’t have such information for all the restaurants, and we are left to make predictions based on the small sample of data points.\n",
    "\n",
    "In this task, you are going to predict whether a set of restaurants will pass the public health inspection tests given the corresponding Yelp text reviews along with some additional information such as the locations and cuisines offered in these restaurants. Making a prediction about an unobserved attribute using data mining techniques represents a wide range of important applications of data mining. Through working on this task, you will gain direct experience with such an application. Due to the flexibility of using as many indicators for prediction as possible, this would also give you an opportunity to potentially combine many different algorithms you have learned from the courses in the Data Mining Specialization to solve a real world problem and experiment with different methods to understand what’s the most effective way of solving the problem.\n",
    "\n",
    "## About the Dataset\n",
    "You should first download the [dataset](https://d396qusza40orc.cloudfront.net/dataminingcapstone/Task6/Hygiene.tar.gz). The dataset is composed of a training subset containing 546 restaurants used for training your classifier, in addition to a testing subset of 12753 restaurants used for evaluating the performance of the classifier. In the training subset, you will be provided with a binary label for each restaurant, which indicates whether the restaurant has passed the latest public health inspection test or not, whereas for the testing subset, you will not have access to any labels. The dataset is spread across three files such that the first 546 lines in each file corresponding to the training subset, and the rest are part of the testing subset. Below is a description of each file:\n",
    "\n",
    "* **hygiene.dat**: Each line contains the concatenated text reviews of one restaurant.  \n",
    "* **hygiene.dat.labels**: For the first 546 lines, a binary label (0 or 1) is used where a 0 indicates that the restaurant has passed the latest public health inspection test, while a 1 means that the restaurant has failed the test. The rest of the lines have \"[None]\" in their label field implying that they are part of the testing subset.\n",
    "* **hygiene.dat.additional**: It is a CSV (Comma-Separated Values) file where the first value is a list containing the cuisines offered, the second value is the zip code, which gives an idea about the location, the third is the number of reviews, and the fourth is the average rating, which can vary between 0 and 5 (5 being the best).\n",
    "\n",
    "For testing, we use the F1 measure, which is the harmonic mean of precision and recall, to rank the submissions in the leaderboard. The F1 measure will be based on the macro-averages of precision and recall (macro-averaging is used here to ensure that the two classes are given equal weight as we do not want class 0 to dominate the measure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hygiene_text_path= \"./Hygiene/hygiene.dat\"\n",
    "hygiene_labels_path= \"./Hygiene/hygiene.dat.labels\"\n",
    "hygiene_additional_path= \"./Hygiene/hygiene.dat.additional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from UtilWordEmbedding import MeanEmbeddingVectorizer\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_numeric, strip_short\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces, strip_non_alphanum, remove_stopwords, stem_text\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "SEED=26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Download Glove Embeddings\n",
    "```\n",
    "wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "unzip glove.6B.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Prep\n",
    "Preprocessing Steps:\n",
    "* tokenization and cleaning\n",
    "* stopword removal\n",
    "* stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and preprocess\n",
    "# https://radimrehurek.com/gensim/parsing/preprocessing.html\n",
    "FILTERS_LIST = [lambda x: x.lower(), # lowercase  \n",
    "                strip_tags, # remove tags\n",
    "                strip_punctuation, # replace punctuation characters with spaces\n",
    "                strip_multiple_whitespaces, # remove repeating whitespaces\n",
    "                # strip_numeric, # remove numbers\n",
    "                gensim.parsing.preprocessing.remove_stopwords, # remove stopwords\n",
    "                strip_short, # remove words less than minsize=3 characters long]\n",
    "                stem_text]\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, \n",
    "    \"\"\"\n",
    "    result_stemmed = []\n",
    "    for token in gensim.parsing.preprocessing.preprocess_string(text, FILTERS_LIST):\n",
    "        result_stemmed.append(WordNetLemmatizer().lemmatize(token))\n",
    "    return result_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13299/13299 [01:33<00:00, 142.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 732 ms, total: 1min 33s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = []\n",
    "preprocessed_texts = []\n",
    "\n",
    "with open(hygiene_text_path) as f:\n",
    "    texts = f.readlines()\n",
    "    \n",
    "for _text in tqdm(texts):\n",
    "    result_stemmed = preprocess(_text)\n",
    "    preprocessed_texts.append(result_stemmed)\n",
    "    \n",
    "all_preprocessed_texts = [\" \".join(_text) for _text in preprocessed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13299 entries, 0 to 13298\n",
      "Data columns (total 8 columns):\n",
      "label                 13299 non-null object\n",
      "text                  13299 non-null object\n",
      "preprocessed_texts    13299 non-null object\n",
      "tokenized_texts       13299 non-null object\n",
      "cuisines_offered      13299 non-null object\n",
      "zipcode               13299 non-null object\n",
      "num_reviews           13299 non-null object\n",
      "avg_rating            13299 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 831.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>tokenized_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The baguettes and rolls are excellent, and alt...</td>\n",
       "      <td>baguett roll excel haven tri excit dozen plu t...</td>\n",
       "      <td>[baguett, roll, excel, haven, tri, excit, doze...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I live up the street from Betty. &amp;#160;When my...</td>\n",
       "      <td>live street betti 160 sister town spring break...</td>\n",
       "      <td>[live, street, betti, 160, sister, town, sprin...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm worried about how I will review this place...</td>\n",
       "      <td>worri review place strongli think bad night pl...</td>\n",
       "      <td>[worri, review, place, strongli, think, bad, n...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Why can't you access them on Google street vie...</td>\n",
       "      <td>access googl street view like medina yarrow po...</td>\n",
       "      <td>[access, googl, street, view, like, medina, ya...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Things to like about this place: homemade guac...</td>\n",
       "      <td>thing like place homemad guacamol varieti tast...</td>\n",
       "      <td>[thing, like, place, homemad, guacamol, variet...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text                                 preprocessed_texts                                    tokenized_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0     1  The baguettes and rolls are excellent, and alt...  baguett roll excel haven tri excit dozen plu t...  [baguett, roll, excel, haven, tri, excit, doze...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1     1  I live up the street from Betty. &#160;When my...  live street betti 160 sister town spring break...  [live, street, betti, 160, sister, town, sprin...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2     1  I'm worried about how I will review this place...  worri review place strongli think bad night pl...  [worri, review, place, strongli, think, bad, n...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3     0  Why can't you access them on Google street vie...  access googl street view like medina yarrow po...  [access, googl, street, view, like, medina, ya...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4     0  Things to like about this place: homemade guac...  thing like place homemad guacamol varieti tast...  [thing, like, place, homemad, guacamol, variet...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 546\n",
    "\n",
    "# labels \n",
    "with open(hygiene_labels_path, 'r') as f:\n",
    "    labels = [l.rstrip() for l in f]\n",
    "\n",
    "# texts = []\n",
    "# with open(hygiene_text_path, 'r') as f:\n",
    "#     texts = f.read().splitlines(True)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"label\":labels, \"text\": texts, \n",
    "                   \"preprocessed_texts\": all_preprocessed_texts,\n",
    "                   \"tokenized_texts\": preprocessed_texts})\n",
    "hygiene_additional = pd.read_csv(hygiene_additional_path,  \n",
    "                                 names=[\"cuisines_offered\", \"zipcode\", \"num_reviews\", \"avg_rating\"],\n",
    "                                 dtype={\"cuisines_offered\": str, \n",
    "                                        \"zipcode\": str,\n",
    "                                        \"num_reviews\": str})\n",
    "df = df.join(hygiene_additional)\n",
    "df['avg_rating'] = df['avg_rating'].apply(lambda x: str(int(round(x, 0))))\n",
    "\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 5) (546, 5) (546, 5) (546,)\n",
      "(12753, 5) (12753, 5) (12753, 5) (12753,)\n",
      "text                object\n",
      "cuisines_offered    object\n",
      "zipcode             object\n",
      "num_reviews         object\n",
      "avg_rating          object\n",
      "dtype: object preprocessed_texts    object\n",
      "cuisines_offered      object\n",
      "zipcode               object\n",
      "num_reviews           object\n",
      "avg_rating            object\n",
      "dtype: object tokenized_texts     object\n",
      "cuisines_offered    object\n",
      "zipcode             object\n",
      "num_reviews         object\n",
      "avg_rating          object\n",
      "dtype: object\n",
      "CPU times: user 16.1 ms, sys: 1.95 ms, total: 18.1 ms\n",
      "Wall time: 16.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = df[df[\"label\"] != \"[None]\"]\n",
    "test_df = df[df[\"label\"] == \"[None]\"]\n",
    "\n",
    "additional_feats = [\"cuisines_offered\", \"zipcode\", \"num_reviews\", \"avg_rating\"]\n",
    "\n",
    "train = train_df[[\"text\"] + additional_feats]\n",
    "train_preprocessed = train_df[[\"preprocessed_texts\"] + additional_feats]\n",
    "train_tokenized = train_df[[\"tokenized_texts\"] + additional_feats]\n",
    "train_labels = train_df[\"label\"].astype(int) # needed by sklearn\n",
    "\n",
    "test = test_df[[\"text\"] + additional_feats]\n",
    "test_preprocessed = test_df[[\"preprocessed_texts\"] + additional_feats]\n",
    "test_tokenized = test_df[[\"tokenized_texts\"] + additional_feats]\n",
    "test_labels = test_df[\"label\"]\n",
    "\n",
    "print(train.shape, train_preprocessed.shape, train_tokenized.shape, train_labels.shape)\n",
    "print(test.shape, test_preprocessed.shape, test_tokenized.shape, test_labels.shape)\n",
    "print(train.dtypes, train_preprocessed.dtypes, train_tokenized.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The baguettes and rolls are excellent, and alt...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I live up the street from Betty. &amp;#160;When my...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I'm worried about how I will review this place...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Why can't you access them on Google street vie...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Things to like about this place: homemade guac...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  The baguettes and rolls are excellent, and alt...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  I live up the street from Betty. &#160;When my...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  I'm worried about how I will review this place...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  Why can't you access them on Google street vie...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  Things to like about this place: homemade guac...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>baguett roll excel haven tri excit dozen plu t...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>live street betti 160 sister town spring break...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>worri review place strongli think bad night pl...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>access googl street view like medina yarrow po...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>thing like place homemad guacamol varieti tast...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  preprocessed_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  baguett roll excel haven tri excit dozen plu t...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  live street betti 160 sister town spring break...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  worri review place strongli think bad night pl...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  access googl street view like medina yarrow po...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  thing like place homemad guacamol varieti tast...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[baguett, roll, excel, haven, tri, excit, doze...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[live, street, betti, 160, sister, town, sprin...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[worri, review, place, strongli, think, bad, n...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[access, googl, street, view, like, medina, ya...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[thing, like, place, homemad, guacamol, variet...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     tokenized_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  [baguett, roll, excel, haven, tri, excit, doze...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  [live, street, betti, 160, sister, town, sprin...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  [worri, review, place, strongli, think, bad, n...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  [access, googl, street, view, like, medina, ya...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  [thing, like, place, homemad, guacamol, variet...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.head())\n",
    "display(train_preprocessed.head())\n",
    "display(train_tokenized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just use cross_val_score\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_preprocessed, train_labels, test_size= 0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Experiments\n",
    "\n",
    "Models:  \n",
    "* Naive Bayes\n",
    "* SVM\n",
    "* Logistic Regression\n",
    "* Random Forest\n",
    "* XGBoost\n",
    "\n",
    "Feature Engineering:  \n",
    "* Count Vectorizer\n",
    "* Tfidf Vectorizer\n",
    "* Previously Generated Embeddings\n",
    "    * word2vec\n",
    "    * fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62650104 0.72474747 0.67792317 0.68783693 0.60507246]\n",
      "Average F1-Score: 0.66442\n",
      "CPU times: user 12.4 s, sys: 327 ms, total: 12.7 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(min_df=10), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(max_df=7, token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['avg_rating']),\n",
    "         ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), 'preprocessed_texts')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# scores = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train_preprocessed, train_labels, cv=5, scoring= 'f1_macro')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62857143 0.72222222 0.68965517 0.66037736 0.54347826]\n",
      "Average F1-Score: 0.64886\n",
      "CPU times: user 14.2 s, sys: 360 ms, total: 14.5 s\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(min_df=10), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(max_df=7, token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['avg_rating']),\n",
    "         ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), 'text')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# scores = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1      2193\n",
       "2      1480\n",
       "3      1126\n",
       "4       934\n",
       "5       767\n",
       "6       675\n",
       "7       600\n",
       "8       484\n",
       "9       458\n",
       "10      403\n",
       "11      352\n",
       "12      315\n",
       "13      268\n",
       "14      246\n",
       "16      200\n",
       "15      199\n",
       "17      185\n",
       "18      160\n",
       "19      131\n",
       "20      125\n",
       "21      124\n",
       "22      124\n",
       "23      107\n",
       "24      101\n",
       "25       97\n",
       "28       86\n",
       "26       81\n",
       "27       77\n",
       "29       68\n",
       "30       60\n",
       "32       52\n",
       "33       51\n",
       "37       44\n",
       "36       44\n",
       "31       43\n",
       "34       43\n",
       "35       42\n",
       "39       39\n",
       "44       37\n",
       "38       33\n",
       "43       30\n",
       "47       28\n",
       "42       27\n",
       "40       26\n",
       "45       25\n",
       "46       25\n",
       "41       23\n",
       "54       22\n",
       "59       20\n",
       "49       15\n",
       "52       15\n",
       "63       15\n",
       "55       15\n",
       "62       14\n",
       "48       14\n",
       "51       14\n",
       "73       13\n",
       "57       12\n",
       "61       11\n",
       "50       11\n",
       "56       11\n",
       "53       10\n",
       "58       10\n",
       "66        9\n",
       "78        9\n",
       "89        8\n",
       "83        8\n",
       "67        8\n",
       "60        8\n",
       "65        7\n",
       "70        6\n",
       "64        6\n",
       "93        6\n",
       "77        6\n",
       "69        6\n",
       "84        5\n",
       "85        5\n",
       "112       5\n",
       "76        5\n",
       "100       4\n",
       "91        4\n",
       "74        4\n",
       "68        4\n",
       "80        4\n",
       "105       3\n",
       "72        3\n",
       "86        3\n",
       "107       3\n",
       "136       3\n",
       "101       3\n",
       "122       3\n",
       "88        3\n",
       "75        3\n",
       "142       3\n",
       "132       2\n",
       "90        2\n",
       "110       2\n",
       "177       2\n",
       "94        2\n",
       "197       2\n",
       "102       2\n",
       "148       2\n",
       "116       2\n",
       "125       2\n",
       "144       2\n",
       "161       2\n",
       "92        2\n",
       "71        2\n",
       "123       2\n",
       "130       2\n",
       "81        2\n",
       "95        2\n",
       "171       1\n",
       "167       1\n",
       "169       1\n",
       "118       1\n",
       "147       1\n",
       "127       1\n",
       "129       1\n",
       "243       1\n",
       "111       1\n",
       "203       1\n",
       "232       1\n",
       "98        1\n",
       "99        1\n",
       "141       1\n",
       "260       1\n",
       "189       1\n",
       "119       1\n",
       "162       1\n",
       "181       1\n",
       "108       1\n",
       "253       1\n",
       "103       1\n",
       "210       1\n",
       "251       1\n",
       "96        1\n",
       "134       1\n",
       "234       1\n",
       "176       1\n",
       "135       1\n",
       "139       1\n",
       "268       1\n",
       "109       1\n",
       "193       1\n",
       "97        1\n",
       "568       1\n",
       "145       1\n",
       "104       1\n",
       "233       1\n",
       "128       1\n",
       "79        1\n",
       "259       1\n",
       "214       1\n",
       "121       1\n",
       "188       1\n",
       "266       1\n",
       "106       1\n",
       "Name: num_reviews, dtype: int64"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df['num_reviews'].value_counts()))\n",
    "df['num_reviews'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Thai', 'Restaurants']                                                          640\n",
       "['American (New)', 'Restaurants']                                                596\n",
       "['American (Traditional)', 'Restaurants']                                        589\n",
       "['Mexican', 'Restaurants']                                                       572\n",
       "['Pizza', 'Restaurants']                                                         524\n",
       "['Vietnamese', 'Restaurants']                                                    465\n",
       "['Japanese', 'Sushi Bars', 'Restaurants']                                        459\n",
       "['Sandwiches', 'Restaurants']                                                    430\n",
       "['Chinese', 'Restaurants']                                                       394\n",
       "['Italian', 'Pizza', 'Restaurants']                                              327\n",
       "['Japanese', 'Restaurants']                                                      282\n",
       "['Italian', 'Restaurants']                                                       274\n",
       "['Restaurants']                                                                  237\n",
       "['Fast Food', 'Mexican', 'Restaurants']                                          232\n",
       "['Delis', 'Restaurants']                                                         204\n",
       "['Cafes', 'Sandwiches', 'Restaurants']                                           195\n",
       "['Burgers', 'Fast Food', 'Restaurants']                                          180\n",
       "['Greek', 'Mediterranean', 'Restaurants']                                        177\n",
       "['American (Traditional)', 'Breakfast & Brunch', 'Restaurants']                  168\n",
       "['Fast Food', 'Sandwiches', 'Restaurants']                                       159\n",
       "['Ethiopian', 'Restaurants']                                                     136\n",
       "['Burgers', 'Restaurants']                                                       136\n",
       "['Irish', 'Restaurants']                                                         135\n",
       "['Delis', 'Sandwiches', 'Restaurants']                                           121\n",
       "['Dim Sum', 'Seafood', 'Cantonese', 'Chinese', 'Restaurants']                    118\n",
       "['Seafood', 'Restaurants']                                                        97\n",
       "['Barbeque', 'Restaurants']                                                       91\n",
       "['Italian', 'Pizza', 'Sandwiches', 'Restaurants']                                 86\n",
       "['French', 'Restaurants']                                                         85\n",
       "['Breakfast & Brunch', 'Restaurants']                                             84\n",
       "['Creperies', 'Restaurants']                                                      84\n",
       "['Cafes', 'Restaurants']                                                          84\n",
       "['Indian', 'Restaurants']                                                         80\n",
       "['Breakfast & Brunch', 'Cafes', 'Restaurants']                                    79\n",
       "['Vietnamese', 'Sandwiches', 'Restaurants']                                       77\n",
       "['Delis', 'Vietnamese', 'Sandwiches', 'Restaurants']                              76\n",
       "['Seafood', 'Fast Food', 'Fish & Chips', 'Restaurants']                           69\n",
       "['Breakfast & Brunch', 'American (New)', 'Restaurants']                           68\n",
       "['Indian', 'Buffets', 'Restaurants']                                              67\n",
       "['German', 'Restaurants']                                                         64\n",
       "['Japanese', 'Chinese', 'Restaurants']                                            64\n",
       "['Asian Fusion', 'Chinese', 'Restaurants']                                        63\n",
       "['Barbeque', 'Chinese', 'Restaurants']                                            59\n",
       "['Asian Fusion', 'Restaurants']                                                   53\n",
       "['Breakfast & Brunch', 'Seafood', 'Buffets', 'Restaurants']                       51\n",
       "['Breakfast & Brunch', 'Sandwiches', 'Restaurants']                               51\n",
       "['Mediterranean', 'Restaurants']                                                  49\n",
       "['American (New)', 'French', 'Restaurants']                                       47\n",
       "['Restaurants', 'Pizza']                                                          45\n",
       "['Delis', 'Italian', 'Restaurants']                                               44\n",
       "['Mexican', 'Sandwiches', 'Restaurants']                                          41\n",
       "['American (Traditional)', 'Seafood', 'Restaurants']                              41\n",
       "['British', 'Restaurants']                                                        41\n",
       "['American (Traditional)', 'Soup', 'Seafood', 'Restaurants']                      40\n",
       "['American (Traditional)', 'Steakhouses', 'Restaurants']                          40\n",
       "['Steakhouses', 'Restaurants']                                                    40\n",
       "['Pakistani', 'Indian', 'Restaurants']                                            38\n",
       "['Mexican', 'Tex-Mex', 'Restaurants']                                             38\n",
       "['Vegetarian', 'Vegan', 'Restaurants']                                            37\n",
       "['Filipino', 'Restaurants']                                                       37\n",
       "['Cantonese', 'Chinese', 'Restaurants']                                           37\n",
       "['Tapas Bars', 'Spanish', 'Basque', 'Restaurants']                                36\n",
       "['Tex-Mex', 'Restaurants']                                                        35\n",
       "['Cheesesteaks', 'Sandwiches', 'Restaurants']                                     34\n",
       "['American (Traditional)', 'Burgers', 'Restaurants']                              34\n",
       "['Seafood', 'Steakhouses', 'Restaurants']                                         34\n",
       "['Vietnamese', 'Chinese', 'Restaurants']                                          33\n",
       "['Dim Sum', 'Cantonese', 'Chinese', 'Restaurants']                                31\n",
       "['Mediterranean', 'Middle Eastern', 'Restaurants']                                31\n",
       "['Breakfast & Brunch', 'Diners', 'Restaurants']                                   31\n",
       "['Fast Food', 'Southern', 'Soul Food', 'Restaurants']                             29\n",
       "['Sushi Bars', 'Restaurants']                                                     29\n",
       "['Breakfast & Brunch', 'Vegetarian', 'Restaurants']                               28\n",
       "['American (Traditional)', 'Diners', 'Restaurants']                               28\n",
       "['Greek', 'Mediterranean', 'Middle Eastern', 'Restaurants']                       27\n",
       "['Seafood', 'Cantonese', 'Chinese', 'Restaurants']                                27\n",
       "['American (Traditional)', 'Breakfast & Brunch', 'Diners', 'Restaurants']         27\n",
       "['Burgers', 'American (New)', 'Restaurants']                                      26\n",
       "['Hawaiian', 'Restaurants']                                                       26\n",
       "['Turkish', 'Mediterranean', 'Restaurants']                                       26\n",
       "['Barbeque', 'Chicken Wings', 'Restaurants']                                      26\n",
       "['American (Traditional)', 'American (New)', 'Restaurants']                       25\n",
       "['Middle Eastern', 'Restaurants']                                                 23\n",
       "['Hot Pot', 'Szechuan', 'Chinese', 'Restaurants']                                 23\n",
       "['Sushi Bars', 'Japanese', 'Hawaiian', 'Restaurants']                             23\n",
       "['Cuban', 'Caribbean', 'Sandwiches', 'Restaurants']                               23\n",
       "['Seafood', 'American (New)', 'Restaurants']                                      23\n",
       "['Burgers', 'Gluten-Free', 'Restaurants']                                         23\n",
       "['Gluten-Free', 'Restaurants']                                                    22\n",
       "['Cafes', 'French', 'Restaurants']                                                21\n",
       "['Korean', 'Japanese', 'Chinese', 'Restaurants']                                  21\n",
       "['Dim Sum', 'Chinese', 'Restaurants']                                             21\n",
       "['Cajun/Creole', 'Restaurants']                                                   21\n",
       "['Restaurants', 'Cafes']                                                          21\n",
       "['Seafood', 'Chinese', 'Restaurants']                                             21\n",
       "['Breakfast & Brunch', 'Burgers', 'Restaurants']                                  21\n",
       "['Asian Fusion', 'Sushi Bars', 'Restaurants']                                     20\n",
       "['Korean', 'Japanese', 'Restaurants']                                             20\n",
       "['Diners', 'Restaurants']                                                         20\n",
       "['Indian', 'Himalayan/Nepalese', 'Restaurants']                                   20\n",
       "['Seafood', 'Breakfast & Brunch', 'Restaurants']                                  20\n",
       "['Dim Sum', 'Cantonese', 'Barbeque', 'Chinese', 'Restaurants']                    20\n",
       "['Breakfast & Brunch', 'French', 'Restaurants']                                   19\n",
       "['Breakfast & Brunch', 'American (New)', 'Cafes', 'Restaurants']                  19\n",
       "['Cambodian', 'Restaurants']                                                      19\n",
       "['Modern European', 'American (New)', 'Restaurants']                              18\n",
       "['Greek', 'Restaurants']                                                          18\n",
       "['Russian', 'Restaurants']                                                        18\n",
       "['Brazilian', 'Restaurants']                                                      18\n",
       "['Chinese', 'Dim Sum', 'Seafood', 'Restaurants']                                  18\n",
       "['Pizza', 'Sandwiches', 'Restaurants']                                            18\n",
       "['Tapas Bars', 'American (New)', 'Restaurants']                                   18\n",
       "['Fast Food', 'Japanese', 'Restaurants']                                          18\n",
       "['Pakistani', 'Indian', 'Buffets', 'Restaurants']                                 18\n",
       "['Fast Food', 'Chinese', 'Restaurants']                                           18\n",
       "['Tapas Bars', 'Restaurants']                                                     17\n",
       "['Seafood', 'Italian', 'Restaurants']                                             17\n",
       "['American (New)', 'Sandwiches', 'Restaurants']                                   17\n",
       "['Vegetarian', 'Restaurants']                                                     17\n",
       "['Breakfast & Brunch', 'Creperies', 'Restaurants']                                17\n",
       "['Mexican', 'Salvadoran', 'Latin American', 'Restaurants']                        16\n",
       "['Burgers', 'Hot Dogs', 'Restaurants']                                            16\n",
       "['Vietnamese', 'Sandwiches', 'Japanese', 'Restaurants']                           16\n",
       "['Restaurants', 'Mexican']                                                        16\n",
       "['Korean', 'Restaurants']                                                         16\n",
       "['Asian Fusion', 'Japanese', 'Restaurants']                                       16\n",
       "['American (Traditional)', 'Cafes', 'Restaurants']                                16\n",
       "['American (Traditional)', 'Breakfast & Brunch', 'Burgers', 'Restaurants']        16\n",
       "['Barbeque', 'Soul Food', 'Restaurants']                                          16\n",
       "['Pizza', 'Gluten-Free', 'Restaurants']                                           16\n",
       "['Asian Fusion', 'Japanese', 'Sushi Bars', 'Restaurants']                         15\n",
       "['Pakistani', 'Indian', 'Halal', 'Restaurants']                                   15\n",
       "['Breakfast & Brunch', 'Mexican', 'Restaurants']                                  15\n",
       "['Restaurants', 'Italian']                                                        15\n",
       "['Breakfast & Brunch', 'Vietnamese', 'Restaurants']                               15\n",
       "['Asian Fusion', 'Dim Sum', 'Vietnamese', 'Chinese', 'Restaurants']               15\n",
       "['Thai', 'Vietnamese', 'Restaurants']                                             14\n",
       "['Thai', 'Chinese', 'Restaurants']                                                14\n",
       "['Mediterranean', 'Gastropubs', 'Restaurants']                                    14\n",
       "['Moroccan', 'Restaurants']                                                       14\n",
       "['Asian Fusion', 'Thai', 'Chinese', 'Restaurants']                                14\n",
       "['Taiwanese', 'Chinese', 'Restaurants']                                           14\n",
       "['African', 'Restaurants']                                                        14\n",
       "['Vegetarian', 'Vegan', 'Hot Dogs', 'Restaurants']                                14\n",
       "['Delis', 'Vietnamese', 'Restaurants']                                            14\n",
       "['Asian Fusion', 'Gluten-Free', 'Chinese', 'Restaurants']                         14\n",
       "['Breakfast & Brunch', 'Delis', 'Burgers', 'Restaurants']                         14\n",
       "['Vegetarian', 'Vietnamese', 'Restaurants']                                       14\n",
       "['American (Traditional)', 'Breakfast & Brunch', 'Vegetarian', 'Restaurants']     14\n",
       "['Creperies', 'Sandwiches', 'Restaurants']                                        13\n",
       "['Barbeque', 'Hawaiian', 'Restaurants']                                           13\n",
       "['Pizza', 'Gluten-Free', 'Vegan', 'Restaurants']                                  13\n",
       "['Italian', 'Pizza', 'Cafes', 'Restaurants']                                      13\n",
       "['Breakfast & Brunch', 'Seafood', 'American (New)', 'Restaurants']                13\n",
       "['American (Traditional)', 'Breakfast & Brunch', 'Seafood', 'Restaurants']        13\n",
       "['Pizza', 'Fast Food', 'Restaurants']                                             13\n",
       "['Soup', 'Restaurants']                                                           13\n",
       "['German', 'Steakhouses', 'Restaurants']                                          13\n",
       "['German', 'Sandwiches', 'Restaurants']                                           13\n",
       "['Vegetarian', 'Modern European', 'Mediterranean', 'Restaurants']                 13\n",
       "['Mexican', 'Latin American', 'Restaurants']                                      13\n",
       "['Pizza', 'American (New)', 'Restaurants']                                        13\n",
       "['Fondue', 'Gluten-Free', 'Restaurants']                                          13\n",
       "['American (Traditional)', 'Sandwiches', 'Restaurants']                           13\n",
       "['Tapas/Small Plates', 'Mediterranean', 'Restaurants']                            13\n",
       "['Sandwiches', 'French', 'Restaurants']                                           13\n",
       "['Gastropubs', 'Restaurants']                                                     13\n",
       "['Shanghainese', 'Chinese', 'Restaurants']                                        13\n",
       "['American (New)', 'Diners', 'Restaurants']                                       12\n",
       "['Creperies', 'Food Stands', 'Restaurants']                                       12\n",
       "['Southern', 'Restaurants']                                                       12\n",
       "['Szechuan', 'Chinese', 'Restaurants']                                            12\n",
       "['Burgers', 'Breakfast & Brunch', 'Restaurants']                                  12\n",
       "['American (Traditional)', 'Burgers', 'Sandwiches', 'Restaurants']                12\n",
       "['Salvadoran', 'Latin American', 'Restaurants']                                   12\n",
       "['Burgers', 'Sandwiches', 'Restaurants']                                          12\n",
       "['Hot Dogs', 'Restaurants']                                                       12\n",
       "['Mongolian', 'Restaurants']                                                      12\n",
       "['Mediterranean', 'Italian', 'Pizza', 'Restaurants']                              12\n",
       "['Barbeque', 'Southern', 'Restaurants']                                           12\n",
       "['Sushi Bars', 'Japanese', 'Chinese', 'Restaurants']                              12\n",
       "['Gastropubs', 'American (New)', 'Restaurants']                                   12\n",
       "['Modern European', 'Sandwiches', 'Restaurants']                                  12\n",
       "['Burgers', 'Seafood', 'Fish & Chips', 'Restaurants']                             11\n",
       "['American (Traditional)', 'Breakfast & Brunch', 'Cafes', 'Restaurants']          11\n",
       "['Burgers', 'Pizza', 'Restaurants']                                               11\n",
       "['Persian/Iranian', 'Restaurants']                                                11\n",
       "['Afghan', 'Restaurants']                                                         11\n",
       "['Live/Raw Food', 'Vegan', 'Restaurants']                                         11\n",
       "['Trinidadian', 'Caribbean', 'Restaurants']                                       11\n",
       "['Restaurants', 'American (New)']                                                 11\n",
       "['Breakfast & Brunch', 'Delis', 'Restaurants']                                    11\n",
       "['Delis', 'Turkish', 'Restaurants']                                               11\n",
       "['Vegetarian', 'Ethiopian', 'Vegan', 'Restaurants']                               11\n",
       "['Malaysian', 'Restaurants']                                                      11\n",
       "['Modern European', 'Restaurants', 'Belgian']                                     11\n",
       "['Chinese', 'Dim Sum', 'Restaurants']                                             11\n",
       "['Indian', 'Mediterranean', 'Restaurants']                                        11\n",
       "['Modern European', 'Restaurants']                                                11\n",
       "['Korean', 'Japanese', 'Sushi Bars', 'Restaurants']                               11\n",
       "['Seafood', 'French', 'Restaurants']                                              11\n",
       "['Food Stands', 'Hot Dogs', 'Restaurants']                                        11\n",
       "['Caribbean', 'Haitian', 'Restaurants']                                           11\n",
       "['Mediterranean', 'Greek', 'Restaurants']                                         11\n",
       "['Breakfast & Brunch', 'Cafes', 'Ethiopian', 'Restaurants']                       11\n",
       "['Greek', 'American (New)', 'Restaurants']                                        10\n",
       "['Fast Food', 'Food Stands', 'Restaurants']                                       10\n",
       "['Restaurants', 'Sandwiches']                                                     10\n",
       "['Greek', 'Pizza', 'Mediterranean', 'Restaurants']                                10\n",
       "['Cajun/Creole', 'Diners', 'Restaurants']                                         10\n",
       "['Soup', 'Delis', 'Restaurants']                                                  10\n",
       "['Mediterranean', 'Moroccan', 'Restaurants']                                      10\n",
       "['Fast Food', 'Buffets', 'Chinese', 'Restaurants']                                10\n",
       "['Seafood', 'Japanese', 'Sushi Bars', 'Restaurants']                              10\n",
       "['Seafood', 'Live/Raw Food', 'Restaurants']                                       10\n",
       "['Vietnamese', 'Food Court', 'Sandwiches', 'Restaurants']                         10\n",
       "['Vegetarian', 'Indian', 'Restaurants']                                           10\n",
       "['Italian', 'Basque', 'Spanish', 'Restaurants']                                   10\n",
       "['Polish', 'Delis', 'Restaurants']                                                10\n",
       "['Thai', 'Vegetarian', 'Restaurants']                                              9\n",
       "['Cantonese', 'Barbeque', 'Chinese', 'Restaurants']                                9\n",
       "['Vegetarian', 'Gluten-Free', 'Vegan', 'Restaurants']                              9\n",
       "['Gluten-Free', 'Japanese', 'Sushi Bars', 'Restaurants']                           9\n",
       "['Seafood', 'Fish & Chips', 'Restaurants']                                         9\n",
       "['American (Traditional)', 'Chinese', 'Restaurants']                               9\n",
       "['Vietnamese', 'Japanese', 'Restaurants']                                          9\n",
       "['Burgers', 'Seafood', 'Restaurants']                                              9\n",
       "['Fast Food', 'Hot Dogs', 'Restaurants']                                           9\n",
       "['Asian Fusion', 'Vegetarian', 'Delis', 'Restaurants']                             9\n",
       "['Seafood', 'Diners', 'Restaurants']                                               9\n",
       "['German', 'Fast Food', 'Restaurants']                                             9\n",
       "['Delis', 'Mexican', 'Restaurants']                                                9\n",
       "['Barbeque', 'Korean', 'Food Court', 'Restaurants']                                9\n",
       "['Russian', 'Cafes', 'Restaurants']                                                8\n",
       "['Spanish', 'Restaurants']                                                         8\n",
       "['Asian Fusion', 'Korean', 'Sandwiches', 'Restaurants']                            8\n",
       "['Delis', 'Buffets', 'Restaurants']                                                8\n",
       "['Dim Sum', 'Taiwanese', 'Chinese', 'Restaurants']                                 8\n",
       "['American (New)', 'Breakfast & Brunch', 'Restaurants']                            8\n",
       "['Cantonese', 'Vietnamese', 'Chinese', 'Restaurants']                              8\n",
       "['Seafood', 'Southern', 'Soul Food', 'Restaurants']                                8\n",
       "['Southern', 'Soul Food', 'Restaurants']                                           8\n",
       "['Vegetarian', 'Vietnamese', 'Delis', 'Restaurants']                               8\n",
       "['American (New)', 'Tapas/Small Plates', 'Restaurants']                            8\n",
       "['Asian Fusion', 'Tapas Bars', 'Thai', 'Restaurants']                              8\n",
       "['Fast Food', 'Restaurants']                                                       8\n",
       "['Asian Fusion', 'Buffets', 'Restaurants']                                         8\n",
       "['Buffets', 'Salad', 'Restaurants']                                                8\n",
       "['Soup', 'Sandwiches', 'Restaurants']                                              7\n",
       "['Turkish', 'Sandwiches', 'Middle Eastern', 'Restaurants']                         7\n",
       "['Burgers', 'Cheesesteaks', 'Barbeque', 'Restaurants']                             7\n",
       "['Greek', 'Mediterranean', 'French', 'Restaurants']                                7\n",
       "['Breakfast & Brunch', 'Cafes', 'Vegan', 'Restaurants']                            7\n",
       "['Asian Fusion', 'Indonesian', 'Restaurants']                                      7\n",
       "['Pizza', 'Italian', 'Fast Food', 'Restaurants']                                   7\n",
       "['Chinese', 'Sushi Bars', 'Restaurants']                                           7\n",
       "['Indonesian', 'Taiwanese', 'Chinese', 'Restaurants']                              7\n",
       "['Vegetarian', 'Ethiopian', 'Restaurants']                                         7\n",
       "['Asian Fusion', 'Korean', 'Hawaiian', 'Restaurants']                              7\n",
       "['Thai', 'Japanese', 'Sushi Bars', 'Restaurants']                                  7\n",
       "['Indian', 'Tapas/Small Plates', 'American (New)', 'Restaurants']                  7\n",
       "['Tapas/Small Plates', 'Restaurants']                                              7\n",
       "['Vegan', 'Restaurants']                                                           7\n",
       "['Korean', 'Barbeque', 'Chicken Wings', 'Restaurants']                             7\n",
       "['Scandinavian', 'Restaurants']                                                    7\n",
       "['Cafes', 'Gluten-Free', 'Restaurants']                                            7\n",
       "['Buffets', 'Restaurants']                                                         7\n",
       "['Caribbean', 'Restaurants']                                                       7\n",
       "['Thai', 'Laotian', 'Restaurants']                                                 7\n",
       "['Barbeque', 'Vietnamese', 'Chinese', 'Restaurants']                               7\n",
       "['American (Traditional)', 'Soup', 'Sandwiches', 'Restaurants']                    6\n",
       "['Tapas Bars', 'Italian', 'Restaurants']                                           6\n",
       "['Pizza', 'Vegan', 'Restaurants']                                                  6\n",
       "['American (Traditional)', 'Soup', 'Restaurants']                                  6\n",
       "['Vegetarian', 'Thai', 'Vegan', 'Restaurants']                                     6\n",
       "['Burgers', 'Fish & Chips', 'Restaurants']                                         6\n",
       "['Breakfast & Brunch', 'Greek', 'Restaurants']                                     6\n",
       "['Cafes', 'Mediterranean', 'Restaurants']                                          6\n",
       "['Barbeque', 'American (New)', 'Restaurants']                                      6\n",
       "['Gluten-Free', 'Vegan', 'Restaurants']                                            6\n",
       "['Asian Fusion', 'Vietnamese', 'Restaurants']                                      6\n",
       "['Vietnamese', 'French', 'Restaurants']                                            6\n",
       "['Fast Food', 'American (Traditional)', 'Restaurants']                             6\n",
       "['Halal', 'Restaurants']                                                           6\n",
       "['Food Stands', 'Mexican', 'Restaurants']                                          6\n",
       "['Breakfast & Brunch', 'Burgers', 'American (New)', 'Restaurants']                 6\n",
       "['Asian Fusion', 'Chicken Wings', 'Chinese', 'Restaurants']                        6\n",
       "['Pizza', 'Mediterranean', 'Restaurants']                                          6\n",
       "['Korean', 'Fast Food', 'Japanese', 'Restaurants']                                 6\n",
       "['Vegetarian', 'Live/Raw Food', 'Restaurants']                                     5\n",
       "['Barbeque', 'Mediterranean', 'Restaurants']                                       5\n",
       "['Chicken Wings', 'Restaurants']                                                   5\n",
       "['Fast Food', 'Chicken Wings', 'Restaurants']                                      5\n",
       "['Seafood', 'Indian', 'Restaurants']                                               5\n",
       "['Tapas/Small Plates', 'Breakfast & Brunch', 'Restaurants']                        5\n",
       "['Breakfast & Brunch', 'Sandwiches', 'Filipino', 'Restaurants']                    5\n",
       "['Lebanese', 'Mediterranean', 'Middle Eastern', 'Restaurants']                     5\n",
       "['Italian', 'American (New)', 'Restaurants']                                       5\n",
       "['Turkish', 'Italian', 'Mediterranean', 'Restaurants']                             5\n",
       "['Tapas Bars', 'Restaurants', 'Pizza']                                             5\n",
       "['Vegan', 'Live/Raw Food', 'Restaurants']                                          5\n",
       "['Breakfast & Brunch', 'Italian', 'Pizza', 'Restaurants']                          5\n",
       "['Vegetarian', 'Pizza', 'Vegan', 'Restaurants']                                    5\n",
       "['Seafood', 'American (New)', 'Live/Raw Food', 'Restaurants']                      5\n",
       "['Mexican', 'Caribbean', 'Restaurants']                                            5\n",
       "['Asian Fusion', 'American (New)', 'Restaurants']                                  5\n",
       "['Kosher', 'Burgers', 'Halal', 'Restaurants']                                      5\n",
       "['Vegetarian', 'Pizza', 'Restaurants']                                             4\n",
       "['Food Stands', 'Mexican', 'Salvadoran', 'Latin American', 'Restaurants']          4\n",
       "['Breakfast & Brunch', 'Belgian', 'Restaurants']                                   4\n",
       "['German', 'Cafes', 'Sandwiches', 'Restaurants']                                   4\n",
       "['Burgers', 'Chicken Wings', 'Restaurants']                                        4\n",
       "['Delis', 'Indian', 'Restaurants']                                                 4\n",
       "['Italian', 'Cafes', 'Sandwiches', 'Restaurants']                                  4\n",
       "['Asian Fusion', 'Burgers', 'Restaurants']                                         4\n",
       "['Buffets', 'Chinese', 'Restaurants']                                              4\n",
       "['Belgian', 'Sandwiches', 'Restaurants']                                           4\n",
       "['Puerto Rican', 'Caribbean', 'Restaurants']                                       4\n",
       "['Breakfast & Brunch', 'American (New)', 'Diners', 'Restaurants']                  4\n",
       "['Cafes', 'Delis', 'Restaurants']                                                  4\n",
       "['Delis', 'Japanese', 'Restaurants']                                               4\n",
       "['American (Traditional)', 'Mediterranean', 'Restaurants']                         4\n",
       "['Vietnamese', 'Sandwiches', 'Chinese', 'Restaurants']                             4\n",
       "['Cuban', 'Venezuelan', 'Colombian', 'Latin American', 'Restaurants']              3\n",
       "['Delis', 'Comfort Food', 'Sandwiches', 'Restaurants']                             3\n",
       "['Kosher', 'Vegetarian', 'Vegan', 'Chinese', 'Restaurants']                        3\n",
       "['Cafes', 'Sandwiches', 'Japanese', 'Restaurants']                                 3\n",
       "['Cafes', 'Latin American', 'Restaurants']                                         3\n",
       "['Caribbean', 'Latin American', 'Restaurants']                                     3\n",
       "['Italian', 'Sandwiches', 'Restaurants']                                           3\n",
       "['Creperies', 'Sandwiches', 'Cafes', 'Restaurants']                                3\n",
       "['African', 'Ethiopian', 'Restaurants']                                            3\n",
       "['American (Traditional)', 'Modern European', 'Restaurants']                       3\n",
       "['Seafood', 'Cajun/Creole', 'Soul Food', 'Restaurants']                            3\n",
       "['Himalayan/Nepalese', 'Restaurants']                                              3\n",
       "['Asian Fusion', 'Thai', 'Restaurants']                                            3\n",
       "['Seafood', 'Cajun/Creole', 'Diners', 'Restaurants']                               3\n",
       "['Vietnamese', 'Cafes', 'Restaurants']                                             3\n",
       "['Italian', 'Food Stands', 'Restaurants']                                          3\n",
       "['Burgers', 'Barbeque', 'Restaurants']                                             3\n",
       "['Senegalese', 'African', 'Restaurants']                                           2\n",
       "['Creperies', 'Sandwiches', 'French', 'Restaurants']                               2\n",
       "['Seafood', 'Thai', 'Restaurants']                                                 2\n",
       "['Breakfast & Brunch', 'Cafes', 'Sandwiches', 'Restaurants']                       2\n",
       "['Burgers', 'Pizza', 'American (New)', 'Restaurants']                              2\n",
       "['Kosher', 'Restaurants']                                                          2\n",
       "['Creperies', 'Food Court', 'Restaurants']                                         2\n",
       "['Burgers', 'Japanese', 'Restaurants']                                             2\n",
       "['Lebanese', 'Middle Eastern', 'Restaurants']                                      2\n",
       "['German', 'Gastropubs', 'Hot Dogs', 'Restaurants']                                2\n",
       "['American (New)', 'Cafes', 'Restaurants']                                         2\n",
       "['Pizza', 'Sandwiches', 'Hot Dogs', 'Restaurants']                                 2\n",
       "['Barbeque', 'Sandwiches', 'Restaurants']                                          2\n",
       "['Indonesian', 'Restaurants']                                                      2\n",
       "['Caribbean', 'Cuban', 'Restaurants']                                              2\n",
       "['Asian Fusion', 'Hot Pot', 'Japanese', 'Restaurants']                             2\n",
       "['Fish & Chips', 'Restaurants']                                                    2\n",
       "['Australian', 'Restaurants']                                                      2\n",
       "['Cafes', 'American (New)', 'Restaurants']                                         2\n",
       "['Scottish', 'Restaurants']                                                        2\n",
       "['Vietnamese', 'Tapas Bars', 'Restaurants']                                        2\n",
       "['Cafes', 'Chinese', 'Restaurants']                                                2\n",
       "['Salad', 'Restaurants']                                                           1\n",
       "['Steakhouses', 'American (New)', 'Restaurants']                                   1\n",
       "['Greek', 'Italian', 'Pizza', 'Restaurants']                                       1\n",
       "['Asian Fusion', 'Korean', 'Mexican', 'Restaurants']                               1\n",
       "['Breakfast & Brunch', 'Seafood', 'Restaurants']                                   1\n",
       "['Cheesesteaks', 'Food Stands', 'Hot Dogs', 'Restaurants']                         1\n",
       "['American (New)', 'Salvadoran', 'Latin American', 'Restaurants']                  1\n",
       "['Italian', 'Cafes', 'Restaurants']                                                1\n",
       "['Seafood', 'Cajun/Creole', 'Restaurants']                                         1\n",
       "['Asian Fusion', 'Korean', 'Restaurants']                                          1\n",
       "['Mongolian', 'Japanese', 'Restaurants']                                           1\n",
       "['American (Traditional)', 'Delis', 'Restaurants']                                 1\n",
       "['Steakhouses', 'Brazilian', 'Restaurants']                                        1\n",
       "['Comfort Food', 'Restaurants']                                                    1\n",
       "['Korean', 'Cafes', 'Restaurants']                                                 1\n",
       "['Sandwiches', 'Delis', 'Restaurants']                                             1\n",
       "['American (Traditional)', 'Seafood', 'Fish & Chips', 'Restaurants']               1\n",
       "['Mongolian', 'Chinese', 'Restaurants']                                            1\n",
       "['Persian/Iranian', 'Sandwiches', 'Middle Eastern', 'Restaurants']                 1\n",
       "['Cafes', 'Southern', 'Restaurants']                                               1\n",
       "['African', 'Mediterranean', 'Restaurants']                                        1\n",
       "['Chinese', 'Dim Sum', 'Restaurants', 'Barbeque']                                  1\n",
       "['Spanish', 'Mediterranean', 'Moroccan', 'Restaurants']                            1\n",
       "['Thai', 'Pakistani', 'Restaurants']                                               1\n",
       "['American (Traditional)', 'Vegetarian', 'Vegan', 'Restaurants']                   1\n",
       "['Egyptian', 'Mediterranean', 'Middle Eastern', 'Restaurants']                     1\n",
       "Name: cuisines_offered, dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df['cuisines_offered'].value_counts()))\n",
    "df['cuisines_offered'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(df['avg_rating'].value_counts()))\n",
    "print(len(df['zipcode'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63551402 0.7027027  0.65486726 0.63636364 0.52272727]\n",
      "Average F1-Score: 0.63043\n",
      "CPU times: user 11.5 s, sys: 420 ms, total: 12 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "         ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), 'preprocessed_texts')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# scores = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train_preprocessed, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', CountVectorizer(token_pattern='\\d+'), 'avg_rating'),\n",
    "         ('text', TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                strip_accents='unicode',\n",
    "                min_df=15,\n",
    "                max_df=0.5,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500), 'text')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# score = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Function for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 7 µs, total: 14 µs\n",
      "Wall time: 16.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def test_classifier(clf, X, y, vectorizer, text_col='text'):\n",
    "    pipeline = Pipeline([\n",
    "        ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(min_df=10), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(max_df=7, token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['avg_rating']),\n",
    "         ('text', vectorizer, text_col)],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "        ('clf', clf)\n",
    "    ], verbose=False)\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring= 'f1_macro')\n",
    "    print(clf)\n",
    "    print(scores)\n",
    "    cv_score = np.average(scores)\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Support Vector Machine': svm.SVC(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=SEED, n_estimators=500, n_jobs=-1),\n",
    "    #'Gradient Boosting': GradientBoostingClassifier()\n",
    "    'XGBoost': XGBClassifier(n_estimators=500, \n",
    "                            max_depth=5, \n",
    "                            learning_rate=0.2, \n",
    "                            objective='binary:logistic',\n",
    "                            scale_pos_weight=2,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=SEED)\n",
    "}\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500)\n",
    "bow = CountVectorizer(\n",
    "                stop_words=STOP_WORDS,\n",
    "                strip_accents='unicode',\n",
    "                min_df=15,\n",
    "                max_df=0.5,\n",
    "                ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 BOW - No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.59880637 0.72456199 0.67879094 0.64220183 0.60911885]\n",
      "Naive Bayes: 0.650695997690299\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.61818813 0.57806452 0.58209082 0.59444493 0.54821774]\n",
      "Support Vector Machine: 0.5842012268003629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.57269196 0.58365164 0.60497261 0.63274933 0.6292517 ]\n",
      "Logistic Regression: 0.6046634471652974\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=-1, oob_score=False, random_state=26, verbose=0,\n",
      "                       warm_start=False)\n",
      "[0.6979782  0.58365164 0.58492003 0.62066039 0.57794613]\n",
      "Random Forest: 0.613031277486723\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=26, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=2, subsample=None,\n",
      "              tree_method=None, validate_parameters=False, verbosity=None)\n",
      "[0.62199313 0.58659924 0.52957603 0.62271    0.64111439]\n",
      "XGBoost: 0.6003985603242331\n",
      "CPU times: user 1min 57s, sys: 4.17 s, total: 2min 1s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train, train_labels, \n",
    "                               vectorizer=bow, text_col='text')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 BOW - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.60750145 0.72474747 0.69714574 0.63225371 0.60215601]\n",
      "Naive Bayes: 0.6527608791811271\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.56468209 0.58368056 0.57806452 0.60507246 0.52813853]\n",
      "Support Vector Machine: 0.5719276299198996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.56363636 0.55510204 0.61438679 0.66009271 0.65766913]\n",
      "Logistic Regression: 0.6101774069626231\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=-1, oob_score=False, random_state=26, verbose=0,\n",
      "                       warm_start=False)\n",
      "[0.6613695  0.59050546 0.63050847 0.61923831 0.6513468 ]\n",
      "Random Forest: 0.6305937089818883\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=26, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=2, subsample=None,\n",
      "              tree_method=None, validate_parameters=False, verbosity=None)\n",
      "[0.64187328 0.60537173 0.56647203 0.64634563 0.54093666]\n",
      "XGBoost: 0.6001998646039552\n",
      "CPU times: user 1min 35s, sys: 3.18 s, total: 1min 38s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train_preprocessed, train_labels, \n",
    "                               vectorizer=bow, text_col='preprocessed_texts')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TFIDF - No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.6447205  0.72474747 0.668357   0.66947439 0.60507246]\n",
      "Naive Bayes: 0.66247436538252\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.63527851 0.72474747 0.66902834 0.62271    0.62594372]\n",
      "Support Vector Machine: 0.6555416107222085\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.63588216 0.69714574 0.66902834 0.63274933 0.63916476]\n",
      "Logistic Regression: 0.6547940650277616\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=-1, oob_score=False, random_state=26, verbose=0,\n",
      "                       warm_start=False)\n",
      "[0.63624339 0.668357   0.65111186 0.60497261 0.6120339 ]\n",
      "Random Forest: 0.634543750174156\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=26, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=2, subsample=None,\n",
      "              tree_method=None, validate_parameters=False, verbosity=None)\n",
      "[0.57277947 0.48694908 0.61464646 0.61308316 0.56822587]\n",
      "XGBoost: 0.5711368099561672\n",
      "CPU times: user 1min 27s, sys: 3.43 s, total: 1min 31s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train, train_labels, \n",
    "                               vectorizer=tfidf, text_col='text')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TFIDF - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.62650104 0.72474747 0.67792317 0.68783693 0.60507246]\n",
      "Naive Bayes: 0.6644162150542321\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0.61704244 0.72474747 0.65951878 0.59547908 0.62594372]\n",
      "Support Vector Machine: 0.6445463003313365\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.65408805 0.72456199 0.69683944 0.60497261 0.61754386]\n",
      "Logistic Regression: 0.6596011913654566\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=-1, oob_score=False, random_state=26, verbose=0,\n",
      "                       warm_start=False)\n",
      "[0.61805556 0.68678161 0.60537173 0.65111186 0.66009271]\n",
      "Random Forest: 0.6442826922213036\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=26, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=2, subsample=None,\n",
      "              tree_method=None, validate_parameters=False, verbosity=None)\n",
      "[0.60879993 0.51966102 0.54055914 0.60430561 0.62372653]\n",
      "XGBoost: 0.5794104470041936\n",
      "CPU times: user 1min 17s, sys: 3.09 s, total: 1min 20s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train_preprocessed, train_labels, \n",
    "                               vectorizer=tfidf, text_col='preprocessed_texts')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Additional Attempts\n",
    "Word Embeddings: Gensim word2vec\n",
    "Ensemble: mlens\n",
    "\n",
    "To try in future:\n",
    "* Undersampling\n",
    "* fasttext\n",
    "* BERT\n",
    "* Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/uiuc/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  if __name__ == '__main__':\n",
      "WARNING:root:cannot compute average owing to no vector for []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 1.21 s, total: 2min 29s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "print(num_workers)\n",
    "\n",
    "word_model = Word2Vec(preprocessed_texts, \n",
    "                      size=300, window=5, min_count=3, \n",
    "                      workers=num_workers)\n",
    "w2v = {w: vec for w, vec in zip(word_model.wv.index2word, word_model.wv.syn0)}\n",
    "mean_vec_tr = MeanEmbeddingVectorizer(word_model)\n",
    "w2v_embeddings = mean_vec_tr.transform(preprocessed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 300)\n",
      "(12753, 300)\n",
      "(546, 8)\n",
      "(12753, 8)\n"
     ]
    }
   ],
   "source": [
    "w2v_train = w2v_embeddings[:546]\n",
    "w2v_test = w2v_embeddings[546:]\n",
    "print(w2v_train.shape)\n",
    "print(w2v_test.shape)\n",
    "      \n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57425743 0.66       0.67272727 0.64       0.5625    ]\n",
      "Average F1-Score: 0.62190\n",
      "CPU times: user 356 ms, sys: 1.86 ms, total: 358 ms\n",
      "Wall time: 357 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_train = w2v_embeddings[:546]\n",
    "clf = svm.SVC()# XGBClassifier(n_estimators=500, n_jobs=-1)\n",
    "scores = cross_val_score(clf, w2v_train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61946903 0.65517241 0.61538462 0.65       0.56074766]\n",
      "Average F1-Score: 0.62015\n",
      "CPU times: user 11.7 s, sys: 18.6 ms, total: 11.7 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_train = w2v_embeddings[:546]\n",
    "params = {'max_depth': 6, \n",
    "         'eta': 0.3, \n",
    "         'objective': 'binary:logistic', \n",
    "         'subsample':0.8, \n",
    "         'n_estimators': 500,\n",
    "         'scale_pos_weight': 2,\n",
    "         'eval_metric': 'auc',\n",
    "         'n_jobs': -1}\n",
    "clf = XGBClassifier(n_estimators=500, \n",
    "                    max_depth=5, \n",
    "                    learning_rate=0.2, \n",
    "                    objective='binary:logistic',\n",
    "                    scale_pos_weight=2,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=SEED)\n",
    "scores = cross_val_score(clf, w2v_train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Ensemble Using mlens\n",
    "* https://mlens.readthedocs.io/en/0.1.x/ensemble_tutorial/#ensemble-model-selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1(y, p): return f1_score(y, p, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=2,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
       "   random_state=4917, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, folds=2, raise_on_ex...108400>)],\n",
       "   n_jobs=-1, name='group-38', raise_on_exception=True, transformers=[])],\n",
       "   verbose=0)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=26, sample_size=20,\n",
       "       scorer=<function f1 at 0x1a69108400>, shuffle=False, verbose=False)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Build ---\n",
    "\n",
    "# Passing a scoring function will create cv scores during fitting\n",
    "# the scorer should be a simple function accepting to vectors and returning a scalar\n",
    "ensemble = SuperLearner(scorer=f1, random_state=SEED)\n",
    "\n",
    "# Build the first layer\n",
    "layer1_classifiers = [MultinomialNB(),\n",
    "                      SVC(probability=False),\n",
    "                      LogisticRegression(),\n",
    "                      XGBClassifier(n_estimators=500, \n",
    "                            max_depth=5, \n",
    "                            learning_rate=0.2, \n",
    "                            objective='binary:logistic',\n",
    "                            scale_pos_weight=2,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=SEED),\n",
    "                      RandomForestClassifier(random_state=SEED, n_estimators=500, n_jobs=-1)]\n",
    "ensemble.add(layer1_classifiers)\n",
    "\n",
    "# Attach the final meta estimator\n",
    "ensemble.add_meta(RandomForestClassifier(random_state=SEED, n_estimators=500, n_jobs=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58715596 0.69565217 0.63247863 0.63063063 0.57446809]\n",
      "Average F1-Score: 0.62408\n",
      "CPU times: user 1min 19s, sys: 11.7 s, total: 1min 31s\n",
      "Wall time: 46.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(min_df=10), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(max_df=7, token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['avg_rating']),\n",
    "         ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), 'preprocessed_texts')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', ensemble)\n",
    "], verbose=False)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# score = metrics.f1_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, train_preprocessed, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.2 s, sys: 1.33 s, total: 40.5 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline.fit(train, train_labels)\n",
    "y_pred = pipeline.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path ='./submissions/submission7_mlens1.txt'\n",
    "create_submission(y_pred, submit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_pred, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('jc26\\n')\n",
    "        for label in y_pred:\n",
    "            f.write(str(int(label)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 227 ms, total: 16.5 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(min_df=10), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(max_df=7, token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['avg_rating']),\n",
    "         ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), 'preprocessed_texts')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "pipeline.fit(train_preprocessed, train_labels)\n",
    "y_pred = pipeline.predict(test_preprocessed)\n",
    "# score = metrics.f1_score(y_test, y_pred)\n",
    "# scores = cross_val_score(pipeline, train_preprocessed, train_labels, cv=5, scoring= 'f1_macro')\n",
    "# print(scores)\n",
    "# print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path ='./submissions/submission10_SVM_preprocessed_countVectorizer.txt'\n",
    "create_submission(y_pred, submit_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Check Before Submitting!!!\n",
    "And just run the follwing cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./submissions/submission9_NB_preprocessed_countVectorizer.txt'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission completed successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!python submit.py jc26 {submit_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Method Comparison\n",
    "Tried two or more text representation techniques and two or more learning algorithms and used the additional features; The comparison gives an insight on why some methods perform better than others.\n",
    "\n",
    "* Text Representation: How text is represented as features (for example, unigrams is one technique, bigrams is another)\n",
    "* Learning Algorithm: The classification algorithm used (for example, SVM, Naive Bayes, Logistic Regression, ... )\n",
    "\n",
    "\n",
    "### Best Performing Method\n",
    "* What toolkit was used?\n",
    "* How was text preprocessed? (i.e., stopword removal, stemming, or any data cleaning technique)\n",
    "* How was text represented as features?\n",
    "* What was the learning algorithm used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html#sphx-glr-auto-examples-compose-plot-column-transformer-py\n",
    "* https://towardsdatascience.com/nlp-performance-of-different-word-embeddings-on-text-classification-de648c6262b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
